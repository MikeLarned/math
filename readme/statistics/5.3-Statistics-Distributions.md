### 5.3 Statistics and their Distributions

Observations in a sample are denoted (little x) by $x_1, x_2,...,x_n$.  If we select two samples of size $n$ from the same population, the first sample $x_i$ values will differ from our second sample.  **Before** we know the exact values of $x_1$, we can denote the same with random variables (big X) $X_1,X_2,...,X_N$.  Before implies there is uncertainty in the actual values of our $x_n$

The two samples we've chosen will have variation in the observed values. Sample mean ($\bar{x}$), standard dev ($\sigma$) and other functions (random varialbes) we run against our observed values will in turn also vary.

R sample

Sample mean ($\bar{x}$) is a point estimate (single number on the number line) of the population mean $\mu$. Each of the six samples contain a small amount of error compared to our population.  The lesson here is that using our sample characteristics as estimates of the population, are just estimates.  It would be extremely rare to have the sample estimates match the population exactly.

##### Statistic

Any quantity whose value can be calculated from sample data.  Prior to obtaining the data, we are uncertain of a characteristic
having a specific result, therefore, a statistic is a random variable denoted by uppercase variables.  Lowercase variables represent the calculated or observed value of the statistic.

$\bar{X}$ sample mean before sample selected (before experiment)  
$\bar{x}$ calculated value of statistic (after experiment)

Statistics have probability distributions, referred to as ***Sampling Distributions***.  If you select 5 samples, each sample has its own
variance, mean, standard deviations, fourth spread etc.  Each of these values is a statistic for the sample.  Across all 5 samples, these values vary.  If the samples are random, the variance in our statistics will have probability distributions.

###### Random Samples

rv $X_1$,$X_2$,...,$X_n$ form a random sample of size n if

1. $X_i$'s are independent
2. Each $X_i$ has the same probability distributions

Independent and indentically distributed.  If you sample with replacement, or the population is infinite
in size (conceptually), conditions 1 and 2 are satisfied exactly.  The conditions are approximately statisfied if sampling is
without replacement and the sample size n is much smaller than the population (at most .05 percent of the population).

Sample size n is $n/N \le .05$ and no replacement, approximately satisfies 1 and 2.

### 5.3 - Distribution of the Sample mean

Sample mean of the rv $\{bar}{X}$ is used to draw conclusions about the population mean $\mu$.  Mean and std. dev are from the distribution or population.

$X_1$,$X_2$,...,$X_n$ random sample from distro, mean $\mu$ and std. dev $\sigma$

1.  $E(\bar{X}) = \mu_\bar{x} = \mu$ - Expected values of $\bar{X}$ is centered exactly at the mean of the population from which the sample is selected
2.  $V(\bar{X}) = \sigma_{\frac{2}{x}} = \sigma^2/n$ and $\sigma_{\bar{X}}=\sigma/\sqrt{n}$ - As n increases, variances decreases.  The rv $\bar{X}$ distribution around $\mu$ becomes more concentrated.
3.  $T_o = X_1 + ... + X_n$ (sample total), $E(T_o) = n\mu = \mu_{T_o}$, $V(T_o) = n\sigma^2$, and $\sigma_{T_o} = \sigma\sqrt{n}$ - Sample total becomes more spread as $n$ increases

 $\sigma_{\bar{X}}=\sigma/\sqrt{n}$ - standard error of the mean.  The magnitude of a typical deviation between the sample mean and population mean.

 R - Example

##### Central Limit Theorem


### 6.1 Point Estimation

We often times have sample data and want to draw conclusions about the population. This can occur when we can't look at every data point in a population, so again we estimate.  Remember that a r.v is a function of the data points
in our sample.  We call that r.v. a feature, or parameter of interest denoted $\hat\theta$.

##### Estimator and Point Estimate

1.  Point Estimate - of a feature $\hat\theta$ is a single value that represents a sensible estimated value of the population $\theta$.  
2.  Estimator - the statistic used in the sample to estimate the population statistic.  There can be more than one sensbile estimator for a statistic.

##### Estimating $\mu$

Given a normal distribution with mean $\mu$, there are a number of estimators that give us a point estimate for $\mu$ from the sample.  We can't tell which estimate is closest to the real population mean, however we can evaluate which estimators get closest to the true value when using them on other samples of our r.v.  Here are a few different estimators for pop mean.

1.  Estimator:  $\bar{X}$, Estimate: $\bar{x}$ = $\sum{x_i}/{n}$ (sample mean)
2.  Estimator: $\tilde{X}$, Estimate $\tilde{x} = (n+1)/2$ (odd) or $avg((n/2) + ((n/2)+1))$ (even).  Normal distributions are symmetric, so $\tilde{x} = \bar{x}$
3.  Estimator: $[min(X_i) + max(X_i)]/2$, Estimate:  $[min(x_i) + max(x_i)]/2$ - average of two extremes
4.  Estimator: $\bar{X}_{tr(10)}$ - 10% trimmed mean (elimination of some outlier in the sample)

##### Common Estimators

1.  $\hat\sigma^2 = S^2 = \frac{\sum(x_i - \bar{x})^2}{n-1}$
2.  

##### Estimation Errors

$\hat\theta = \theta +$ error of estimation.  

The point estimate of our feature in many cases varies from the population feature actual value.  Think about taking multiple samples of size n from the population, each sample can have a different sample average $\bar{X}$ that is some distance from the population mean $\mu$.  The difference between $\hat\theta$ and $\theta$ or $\hat\theta - \theta$ is our error conceptually.  (We want to take a squared error to deal with negative numbers).  The closer our error to 0, or the smaller our error, the better fit the estimator is to the population feature.

##### MSE (Mean Squared Error)

$MSE = E[(\hat\theta - \theta)^2]$

Estimators with a smaller MSE should be better than other estimators.  The problem is that MSE will have smaller values for some values of $\theta$ and a larger MSE for other values.  It may not be possible then to find the smallest MSE.

##### Bias

Point estimator $\hat\theta$ is an unbiased estimator if $E(\hat\theta) = 0$.  $E(\hat\theta) - \theta$ is the bias of $\theta$.  See the 6.1-Biased-Estimator.r script to see how an $E(\hat{X}) - \mu$ does not equal 0 on multiple runs of the script.  There is a small error and that error fluctuates as we take different samples from our populatation.
```
p <- rnorm(1000000, mean=0, sd=1)
s <- sample(p,5)

pframe <- data.frame(feature = p)
sframe <- data.frame(feature = s)

plot <- ggplot() +
  geom_density(data=pframe, aes(x=feature), adjust=5) +
  geom_density(data=sframe, aes(x=feature), adjust=5) +
  scale_fill_manual( values = c("red","blue"))

print(plot)
cat(sprintf("Pop Mean %s ", mean(p)))
cat(sprintf("Sample Mean %s ", mean(s)))
```

The estimator $\hat\theta$ is unbiased if its probability distribution equals $\theta$.

##### References
[Markdown Sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#links)  
[Latex Symbols](http://www.artofproblemsolving.com/wiki/index.php/LaTeX:Symbols)  
[Fitting Function to Distributions in R](https://cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf)
[Density Plot with ggplot](http://www.r-bloggers.com/density-plot-with-ggplot/)  
[Cookbook For R - Plotting distributions](http://www.cookbook-r.com/Graphs/Plotting_distributions_(ggplot2)/)  
[Cookbook for R - Multiplot](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/)
